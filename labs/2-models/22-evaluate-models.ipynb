{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ce7019",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# ğŸ›ï¸ | Cora-For-Zava: ëª¨ë¸ ì„ íƒ\n",
    "\n",
    "í™˜ì˜í•©ë‹ˆë‹¤! ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” í‘œì¤€í™”ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ Azure AI í‰ê°€ SDKë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ AI ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ›’ Zava ì‹œë‚˜ë¦¬ì˜¤\n",
    "\n",
    "**Cora**ëŠ” DIY ì• í˜¸ê°€ë¥¼ ìœ„í•œ í™ˆ ì¸í…Œë¦¬ì–´ ìš©í’ˆì„ íŒë§¤í•˜ëŠ” ê°€ìƒì˜ ì†Œë§¤ì—…ì²´ **Zava**ì˜ ê³ ê° ì„œë¹„ìŠ¤ ì±—ë´‡ì…ë‹ˆë‹¤. Coraê°€ ìµœê³ ì˜ ê³ ê° ê²½í—˜ì„ ì œê³µí•˜ë„ë¡ í•˜ë ¤ë©´ ì í•©í•œ ê¸°ë³¸ ëª¨ë¸ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. Azure OpenAIì—ëŠ” GPT-4o, GPT-4o-mini, GPT-4 ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì´ ìˆìœ¼ë¯€ë¡œ, ì†Œë§¤ì—… ì‚¬ìš© ì‚¬ë¡€ì— ê°€ì¥ ì í•©í•œ í’ˆì§ˆ, ì•ˆì „ì„± ë° ì„±ëŠ¥ì˜ ê· í˜•ì„ ì œê³µí•˜ëŠ” ëª¨ë¸ì„ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ ì™„ì„±í•  ê²°ê³¼ë¬¼\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì„ ë§ˆì¹˜ë©´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- âœ… ë¹„êµë¥¼ ìœ„í•´ ì—¬ëŸ¬ Azure OpenAI ëª¨ë¸ì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\n",
    "- âœ… í‰ê°€ë¥¼ ìœ„í•´ í‘œì¤€í™”ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
    "- âœ… ë‚´ì¥ í‰ê°€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ê°„ í‰ê°€ë¥¼ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.\n",
    "- âœ… ì„±ëŠ¥ ì§€í‘œ(í’ˆì§ˆ, ì•ˆì „ì„±, ì§€ì—° ì‹œê°„)ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.\n",
    "- âœ… ëª¨ë¸ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ì •ë³´ì— ê¸°ë°˜í•œ ëª¨ë¸ ì„ íƒ ê²°ì •ì„ ë‚´ë ¸ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ’¡ í•™ìŠµ ë‚´ìš©\n",
    "\n",
    "- í‰ê°€ë¥¼ ìœ„í•´ ì—¬ëŸ¬ ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•\n",
    "- í‰ê°€ë¥¼ ìœ„í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí•˜ëŠ” ë°©ë²•\n",
    "- ë‚´ì¥ í‰ê°€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ëŠ” ë°©ë²•\n",
    "- ëª¨ë¸ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ë¹„êµí•˜ëŠ” ë°©ë²•\n",
    "- Azure AI Foundry ëª¨ë¸ ë¦¬ë”ë³´ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "> **ì°¸ê³ **: ì´ ë…¸íŠ¸ë¶ì€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°°í¬í•˜ê¸° ì „ì— í•„ìˆ˜ì ì¸ ì‚¬ì „ í”„ë¡œë•ì…˜ í‰ê°€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì„ ë¹„êµí•  ì¤€ë¹„ê°€ ë˜ì…¨ë‚˜ìš”? ì‹œì‘í•´ ë´…ì‹œë‹¤! ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6216d",
   "metadata": {},
   "source": [
    "## Step 1: í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "\n",
    "ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë“¤ì€ ì´ì „ ì„¤ì • ë‹¨ê³„ì—ì„œ `.env` íŒŒì¼ì— ì´ë¯¸ êµ¬ì„±ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "- **AZURE_OPENAI_API_KEY**: Azure OpenAI API í‚¤\n",
    "  - ì œê±°í•˜ê³  SystemManagedIdentity ì‚¬ìš© ì˜ˆì •\n",
    "- **AZURE_OPENAI_ENDPOINT**: Azure OpenAI ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸\n",
    "- **AZURE_OPENAI_API_VERSION**: ì‚¬ìš©í•  API ë²„ì „\n",
    "- **AZURE_SUBSCRIPTION_ID**: Azure êµ¬ë… ID\n",
    "- **AZURE_RESOURCE_GROUP**: Azure ë¦¬ì†ŒìŠ¤ ê·¸ë£¹ ì´ë¦„\n",
    "- **AZURE_AI_PROJECT_NAME**: Azure AI Foundry í”„ë¡œì íŠ¸ ì´ë¦„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ff182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Use override=True to reload any changes made to .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Verify all required Azure service credentials are available\n",
    "required_vars = [\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_SUBSCRIPTION_ID\",\n",
    "    \"AZURE_RESOURCE_GROUP\",\n",
    "    \"AZURE_AI_PROJECT_NAME\",\n",
    "    \"AZURE_AI_FOUNDRY_NAME\"\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    raise EnvironmentError(\n",
    "        f\"âŒ Missing environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"âœ… All environment variables configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f7e4a",
   "metadata": {},
   "source": [
    "## Step 2: í‰ê°€í•  ëª¨ë¸ ì •ì˜\n",
    "\n",
    "í‰ê°€í•  ëª¨ë¸ ë°°í¬ ì´ë¦„ ë°°ì—´ì„ êµ¬ì„±í•©ë‹ˆë‹¤. Azure OpenAI ë¦¬ì†ŒìŠ¤ì— ë°°í¬ëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ ëª©ë¡ì—ì„œ ëª¨ë¸ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "> **íŒ**: ëª¨ë¸ì„ ë°°í¬í•˜ê¸° ì „ì— í’ˆì§ˆ, ì•ˆì „ì„±, ë¹„ìš© ë° ì„±ëŠ¥ ì¸¡ë©´ì—ì„œ ë¹„êµí•˜ë ¤ë©´ [Microsoft Foundry ëª¨ë¸ ë¦¬ë”ë³´ë“œ](https://learn.microsoft.com/azure/ai-foundry/how-to/benchmark-model-in-catalog)ë¥¼ í™œìš©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b52db84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to evaluate\n",
    "# Add or remove model deployment names as needed\n",
    "models_to_evaluate = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1\"\n",
    "]\n",
    "\n",
    "print(f\"âœ… Configured {len(models_to_evaluate)} models for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79eab5a",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ë¦¬ë”ë³´ë“œë¥¼ í™œìš©í•œ ëª¨ë¸ ì„ íƒ\n",
    "\n",
    "ì‚¬ìš©ì ì§€ì • í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê¸° ì „í›„ë¡œ Azure AI Foundry ëª¨ë¸ ë¦¬ë”ë³´ë“œë¥¼ í™œìš©í•´ ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "**ë¦¬ë”ë³´ë“œ ì ‘ê·¼ ë°©ë²•:**\n",
    "1. [Microsoft Foundry Portal](https://ai.azure.com)ë¡œ ì´ë™\n",
    "2. ì™¼ìª½ íŒ¨ë„ì—ì„œ **Model catalog** ì„ íƒ\n",
    "3. Model leaderboards ì„¹ì…˜ì—ì„œ **Browse leaderboards** í´ë¦­\n",
    "\n",
    "**ë¹„êµ ê°€ëŠ¥í•œ í•­ëª©:**\n",
    "- **í’ˆì§ˆ ë¦¬ë”ë³´ë“œ**: ì¶”ë¡ , Q&A, ì½”ë”©, ìˆ˜í•™ ê³¼ì œë³„ ì •í™•ë„ ìˆœìœ„\n",
    "- **ì•ˆì „ ë¦¬ë”ë³´ë“œ**: ìœ í•´ ì½˜í…ì¸ ì— ëŒ€í•œ ì €í•­ì„± ìˆœìœ„\n",
    "- **ë¹„ìš© ë¦¬ë”ë³´ë“œ**: ë¹„ìš© íš¨ìœ¨ì„± ìˆœìœ„\n",
    "- **ì„±ëŠ¥ ë¦¬ë”ë³´ë“œ**: ì²˜ë¦¬ëŸ‰ ë° ì§€ì—° ì‹œê°„ ìˆœìœ„\n",
    "- **íŠ¸ë ˆì´ë“œì˜¤í”„ ì°¨íŠ¸**: í’ˆì§ˆ vs ë¹„ìš©, í’ˆì§ˆ vs ì•ˆì „ì„±, í’ˆì§ˆ vs ì²˜ë¦¬ëŸ‰\n",
    "- **ì‹œë‚˜ë¦¬ì˜¤ë³„**: ì±—ë´‡, ì½”ë“œ ìƒì„± ë“± ì‚¬ìš© ì‚¬ë¡€ì— ì í•©í•œ ëª¨ë¸ ì°¾ê¸°\n",
    "\n",
    "ì´ê²ƒì€ `models_to_evaluate` ëª©ë¡ì„ ì¢íˆëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220d3ae",
   "metadata": {},
   "source": [
    "## Step 3: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "\n",
    "í‰ê°€ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹(ì§ˆì˜ì™€ ì˜ˆìƒ ì‘ë‹µ)ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ëª¨ë“  ëª¨ë¸ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09bf07ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was United States found ?</td>\n",
       "      <td>1776</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of finish does the durable eggshell ...</td>\n",
       "      <td>The durable eggshell finish paint has a subtle...</td>\n",
       "      <td>The durable eggshell finish paint has a subtle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What product fits standard paint trays for qui...</td>\n",
       "      <td>Disposable plastic liners that fit standard pa...</td>\n",
       "      <td>The product that fits standard paint trays for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which paint is recommended for kitchens, bathr...</td>\n",
       "      <td>Washable semi-gloss interior paint for kitchen...</td>\n",
       "      <td>The washable semi-gloss interior paint is reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                     When was United States found ?   \n",
       "1                     What is the capital of France?   \n",
       "2  What type of finish does the durable eggshell ...   \n",
       "3  What product fits standard paint trays for qui...   \n",
       "4  Which paint is recommended for kitchens, bathr...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0                                               1776   \n",
       "1                                              Paris   \n",
       "2  The durable eggshell finish paint has a subtle...   \n",
       "3  Disposable plastic liners that fit standard pa...   \n",
       "4  Washable semi-gloss interior paint for kitchen...   \n",
       "\n",
       "                                            response  \n",
       "0                                               1600  \n",
       "1                                              Paris  \n",
       "2  The durable eggshell finish paint has a subtle...  \n",
       "3  The product that fits standard paint trays for...  \n",
       "4  The washable semi-gloss interior paint is reco...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the evaluation dataset\n",
    "dataset_path = \"22-evaluate-models.jsonl\"\n",
    "test_data = []\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"âœ… Loaded {len(test_data)} test examples from {dataset_path}\\n\")\n",
    "\n",
    "# Display as DataFrame for easy viewing\n",
    "df_test_data = pd.DataFrame(test_data)\n",
    "print(\"ğŸ“Š Test Dataset Preview:\")\n",
    "display(df_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144809f8",
   "metadata": {},
   "source": [
    "## Step 4: Azure AI í”„ë¡œì íŠ¸ êµ¬ì„±\n",
    "\n",
    "í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ Azure AI í”„ë¡œì íŠ¸ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48dd47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Create Azure AI project configuration\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group_name = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "\n",
    "# Dynamically construct the Azure AI Foundry project URL\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "# Initialize and verify credential\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Try to get a token to verify authentication\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    print(\"âœ… Azure credentials verified successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Azure credentials not found or expired!\")\n",
    "    print(\"Please run 'az login' in the terminal to authenticate with Azure.\")\n",
    "    raise\n",
    "\n",
    "print(f\"âœ… Azure AI Project configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5d7f4",
   "metadata": {},
   "source": [
    "## Step 5: ëª¨ë¸ êµ¬ì„± ìƒì„±\n",
    "\n",
    "í‰ê°€í•˜ë ¤ëŠ” ê° ëª¨ë¸ì— ëŒ€í•œ ëª¨ë¸ êµ¬ì„± ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b9fa51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configurations for all models\n",
    "model_configs = {}\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    model_configs[model_name] = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=model_name,\n",
    "        credential=DefaultAzureCredential(),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Created configurations for {len(model_configs)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bf7c7",
   "metadata": {},
   "source": [
    "## Step 6: íƒ€ê²Ÿ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "ì§ˆì˜ë¥¼ ë°›ì•„ íŠ¹ì • ëª¨ë¸ì˜ ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ëŠ” í‰ê°€ìê°€ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6472902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "\n",
    "def create_target_function(model_name):\n",
    "    \"\"\"Create a target function for a specific model\"\"\"\n",
    "\n",
    "    def target_function(query: str, ground_truth: str = None, response: str = None) -> dict:\n",
    "        \"\"\"Generate response from the model\"\"\"\n",
    "        client = AzureOpenAI(\n",
    "            azure_ad_token_provider=get_bearer_token_provider(\n",
    "                DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n",
    "            api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "\n",
    "        # Call the model with the query\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions accurately and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"response\": response.choices[0].message.content\n",
    "        }\n",
    "\n",
    "    return target_function\n",
    "\n",
    "\n",
    "print(\"âœ… Target function factory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4174ec8",
   "metadata": {},
   "source": [
    "## Step 7: Evaluator êµ¬ì„±\n",
    "\n",
    "ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  í‰ê°€ìë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. í’ˆì§ˆê³¼ ì•ˆì „ì„± ì§€í‘œë¥¼ ìœ„í•´ [ë‚´ì¥ í‰ê°€ë„êµ¬](https://learn.microsoft.com/azure/ai-foundry/concepts/observability#what-are-evaluators)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "**í’ˆì§ˆ í‰ê°€ì** (AI ì§€ì›):\n",
    "- **Relevance**: ì‘ë‹µì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì ì ˆí•œì§€ í‰ê°€ (1-5 ìŠ¤ì¼€ì¼)\n",
    "- **Coherence**: ì¶œë ¥ì˜ íë¦„ê³¼ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€ (1-5 ìŠ¤ì¼€ì¼)\n",
    "- **Fluency**: ì–¸ì–´ ëŠ¥ë ¥ ë° ë¬¸ë²• ì •í™•ë„ í‰ê°€ (1-5 ìŠ¤ì¼€ì¼)\n",
    "\n",
    "**ì•ˆì „ì„± í‰ê°€ì** (ì»¨í…ì¸  ì•ˆì „):\n",
    "- **Violence**: ì‘ë‹µì—ì„œ í­ë ¥ì  ì»¨í…ì¸  íƒì§€ (0-7, ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì•ˆì „)\n",
    "- **Hate/Unfairness**: ì¦ì˜¤/ë¶ˆê³µì •ì„± íƒì§€ (0-7)\n",
    "- **Self-Harm**: ìí•´ ê´€ë ¨ ë‚´ìš© íƒì§€ (0-7)\n",
    "- **Sexual**: ì„±ì  ì»¨í…ì¸  íƒì§€ (0-7)\n",
    "\n",
    "> **ì°¸ê³ **: ì €í¬ëŠ” ì»¨í…ìŠ¤íŠ¸ë‚˜ ì •ë‹µ ë°ì´í„°ê°€ í•„ìš” ì—†ëŠ” ê´€ë ¨ì„±, ì¼ê´€ì„±, ìœ ì°½ì„± í‰ê°€ ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ ë°ì´í„°ì…‹ì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œ ê·¼ê±°ì„± í‰ê°€ ë„êµ¬ëŠ” ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "> [í‰ê°€ ë©”íŠ¸ë¦­](https://learn.microsoft.com/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics) ë° ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f08aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    ViolenceEvaluator,\n",
    "    HateUnfairnessEvaluator,\n",
    "    SelfHarmEvaluator,\n",
    "    SexualEvaluator\n",
    ")\n",
    "\n",
    "# Create a judge model configuration for evaluators\n",
    "judge_model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=\"gpt-4o\",  # Use a capable model as judge\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# Initialize quality evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config=judge_model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config=judge_model_config)\n",
    "coherence_eval = CoherenceEvaluator(model_config=judge_model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config=judge_model_config)\n",
    "\n",
    "# Initialize safety evaluators (using azure_ai_project_url instead of dictionary)\n",
    "violence_eval = ViolenceEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "hate_unfairness_eval = HateUnfairnessEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "self_harm_eval = SelfHarmEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "sexual_eval = SexualEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "\n",
    "print(\"âœ… Evaluators configured:\")\n",
    "print(\"   Quality: Groundedness, Relevance, Coherence, Fluency\")\n",
    "print(\"   Safety: Violence, Hate/Unfairness, Self-Harm, Sexual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "291e7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single model and single prompt\n",
    "import tempfile\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸ§ª Running configuration test...\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "# Select first model for testing\n",
    "test_model = models_to_evaluate[0]\n",
    "print(f\"\\nğŸ“‹ Test Model: {test_model}\", flush=True)\n",
    "\n",
    "# Create a simple test dataset with one example\n",
    "test_example = {\n",
    "    \"query\": \"What is the capital of France?\",\n",
    "    \"ground_truth\": \"Paris\",\n",
    "    \"response\": \"Paris\"\n",
    "}\n",
    "\n",
    "# Save test example to a temporary file\n",
    "test_file = tempfile.NamedTemporaryFile(\n",
    "    mode='w', suffix='.jsonl', delete=False)\n",
    "test_file.write(json.dumps(test_example) + '\\n')\n",
    "test_file.close()\n",
    "\n",
    "print(f\"ğŸ“ Test Query: {test_example['query']}\", flush=True)\n",
    "\n",
    "try:\n",
    "    # Create target function for test model\n",
    "    test_target_fn = create_target_function(test_model)\n",
    "\n",
    "    # Test the target function\n",
    "    print(\"\\n1ï¸âƒ£ Testing target function...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    test_result = test_target_fn(**test_example)\n",
    "    print(\n",
    "        f\"   âœ… Target function returned: {test_result['response'][:100]}...\", flush=True)\n",
    "\n",
    "    # Test evaluation with minimal evaluators AND portal publishing\n",
    "    print(\"\\n2ï¸âƒ£ Testing evaluation pipeline with portal publishing...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    from azure.ai.evaluation import evaluate\n",
    "\n",
    "    test_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    test_eval_result = evaluate(\n",
    "        data=test_file.name,\n",
    "        target=test_target_fn,\n",
    "        evaluators={\n",
    "            \"relevance\": relevance_eval,\n",
    "            \"coherence\": coherence_eval,\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"default\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${target.response}\",\n",
    "            }\n",
    "        },\n",
    "        # Publish to portal for verification (using URL format)\n",
    "        azure_ai_project=azure_ai_project_url,\n",
    "        evaluation_name=f\"22-evaluate-models-TEST_{test_model}_{test_timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n   âœ… Evaluation completed successfully!\", flush=True)\n",
    "    print(f\"   ğŸ“Š Sample metrics:\", flush=True)\n",
    "    print(\n",
    "        f\"      - Relevance: {test_eval_result['metrics'].get('relevance', 'N/A')}\", flush=True)\n",
    "    print(\n",
    "        f\"      - Coherence: {test_eval_result['metrics'].get('coherence', 'N/A')}\", flush=True)\n",
    "\n",
    "    # Show portal URL if available\n",
    "    if test_eval_result.get('studio_url'):\n",
    "        print(f\"\\n   ğŸŒ View test results in portal:\", flush=True)\n",
    "        print(f\"      {test_eval_result['studio_url']}\", flush=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60, flush=True)\n",
    "    print(\"âœ… Configuration test PASSED! Ready to run full evaluation.\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Configuration test FAILED!\", flush=True)\n",
    "    print(f\"Error: {str(e)}\", flush=True)\n",
    "    print(\"\\nPlease fix the configuration before proceeding to Step 8.\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    sys.stdout.flush()\n",
    "    raise\n",
    "finally:\n",
    "    # Clean up temporary file\n",
    "    import os\n",
    "    if os.path.exists(test_file.name):\n",
    "        os.unlink(test_file.name)\n",
    "    print(\"\\nğŸ§¹ Temporary test file cleaned up.\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3aefe6",
   "metadata": {},
   "source": [
    "## Step 8: í‰ê°€ ì‹¤í–‰\n",
    "\n",
    "ì´ì œ `evaluate()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ê° ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ëŠ” ì„±ëŠ¥, í’ˆì§ˆ, ì•ˆì „ì„±ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì§€í‘œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê° í‰ê°€ ì‘ì—…ì€:\n",
    "- ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì˜ˆì‹œì— ëŒ€í•´ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸\n",
    "- ì‹¬íŒ ëª¨ë¸ì„ ì‚¬ìš©í•´ í’ˆì§ˆ ì§€í‘œ ê³„ì‚°\n",
    "- Azure AI Content Safetyë¡œ ì•ˆì „ì„± í‰ê°€\n",
    "- í‰ê°€ ì‹œê°„ ì¶”ì (ì§€ì—° ì‹œê°„ ëŒ€ë¦¬ ì§€í‘œë¡œ ì‚¬ìš©)\n",
    "- **ê²°ê³¼ë¥¼ Microsoft Foundry í¬í„¸ì— ê²Œì‹œ**í•´ ì‹œê°í™” ê°€ëŠ¥\n",
    "- **ìƒì„¸ ê²°ê³¼ë¥¼ ë¡œì»¬ì— ì €ì¥**í•´ ì˜¤í”„ë¼ì¸ ë¶„ì„ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e3970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 5 test examples from 22-evaluate-models.jsonl\n",
      "\n",
      "ğŸ“Š Test Dataset Preview:\n",
      "âœ… Azure credentials verified successfully!\n",
      "âœ… Azure AI Project configured\n",
      "âœ… Created configurations for 3 models\n",
      "âœ… Target function factory created\n",
      "âœ… Evaluators configured:\n",
      "   Quality: Groundedness, Relevance, Coherence, Fluency\n",
      "   Safety: Violence, Hate/Unfairness, Self-Harm, Sexual\n",
      "ğŸ§ª Running configuration test...\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Test Model: gpt-4o-mini\n",
      "ğŸ“ Test Query: What is the capital of France?\n",
      "\n",
      "1ï¸âƒ£ Testing target function...\n",
      "   âœ… Target function returned: The capital of France is Paris....\n",
      "\n",
      "2ï¸âƒ£ Testing evaluation pipeline with portal publishing...\n",
      "2026-01-07 11:09:19 +0000 281471671587280 execution.bulk     INFO     Finished 1 / 1 lines.\n",
      "2026-01-07 11:09:19 +0000 281471671587280 execution.bulk     INFO     Average execution time for completed lines: 1.47 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"create_target_function__locals__target_function_20260107_110918_344418\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2026-01-07 11:09:18.344418+00:00\"\n",
      "Duration: \"0:00:02.009094\"\n",
      "\n",
      "2026-01-07 11:09:22 +0000 281471034036688 execution.bulk     INFO     Finished 1 / 1 lines.\n",
      "2026-01-07 11:09:22 +0000 281471034036688 execution.bulk     INFO     Average execution time for completed lines: 2.62 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20260107_110920_375377\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2026-01-07 11:09:20.375377+00:00\"\n",
      "Duration: \"0:00:03.008642\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-07 11:09:23 +0000 281471671587280 execution.bulk     INFO     Finished 1 / 1 lines.\n",
      "2026-01-07 11:09:23 +0000 281471671587280 execution.bulk     INFO     Average execution time for completed lines: 3.47 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"coherence_20260107_110920_374449\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2026-01-07 11:09:20.374449+00:00\"\n",
      "Duration: \"0:00:04.017771\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.008642\",\n",
      "        \"completed_lines\": 1,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"coherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:04.017771\",\n",
      "        \"completed_lines\": 1,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "   âœ… Evaluation completed successfully!\n",
      "   ğŸ“Š Sample metrics:\n",
      "      - Relevance: N/A\n",
      "      - Coherence: N/A\n",
      "\n",
      "   ğŸŒ View test results in portal:\n",
      "      https://ai.azure.com/resource/build/evaluation/9f74c298-7787-4df3-8b75-658ba244ae14?wsid=/subscriptions/0ce67698-ac36-4c1c-8188-e8336e25f023/resourceGroups/rg-Ignite-haewons/providers/Microsoft.CognitiveServices/accounts/aoai-2trqdbow7ql7q/projects/proj-2trqdbow7ql7q&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "\n",
      "============================================================\n",
      "âœ… Configuration test PASSED! Ready to run full evaluation.\n",
      "============================================================\n",
      "\n",
      "ğŸ§¹ Temporary test file cleaned up.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create output directory for evaluation results\n",
    "output_dir = \"./22-evaluate-models-results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results for each model\n",
    "evaluation_results = {}\n",
    "\n",
    "print(f\"ğŸš€ Starting evaluation of {len(models_to_evaluate)} models...\")\n",
    "print(f\"   Test dataset size: {len(test_data)} examples\")\n",
    "print(f\"   Output directory: {output_dir}\\n\")\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    print(f\"ğŸ“Š Evaluating model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Create target function for this model\n",
    "        target_fn = create_target_function(model_name)\n",
    "\n",
    "        # Create output path for this evaluation\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_path = os.path.join(\n",
    "            output_dir, f\"22-evaluate-models_{model_name}_{timestamp}\")\n",
    "\n",
    "        # Run evaluation with both portal publishing and local output\n",
    "        result = evaluate(\n",
    "            data=dataset_path,\n",
    "            target=target_fn,\n",
    "            evaluators={\n",
    "                \"relevance\": relevance_eval,\n",
    "                \"coherence\": coherence_eval,\n",
    "                \"fluency\": fluency_eval,\n",
    "                \"violence\": violence_eval,\n",
    "                \"hate_unfairness\": hate_unfairness_eval,\n",
    "                \"self_harm\": self_harm_eval,\n",
    "                \"sexual\": sexual_eval,\n",
    "            },\n",
    "            evaluator_config={\n",
    "                \"default\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"response\": \"${target.response}\",\n",
    "                }\n",
    "            },\n",
    "            # Publish to Azure AI Foundry portal for visualization (using URL format)\n",
    "            azure_ai_project=azure_ai_project_url,\n",
    "            # Save detailed results locally\n",
    "            output_path=output_path,\n",
    "            # Optional: provide evaluation name for easier tracking in portal\n",
    "            evaluation_name=f\"22-evaluate-models_{model_name}_{timestamp}\"\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Store results with both portal and local file information\n",
    "        evaluation_results[model_name] = {\n",
    "            \"metrics\": result[\"metrics\"],\n",
    "            \"evaluation_time\": elapsed_time,\n",
    "            \"studio_url\": result.get(\"studio_url\"),\n",
    "            \"output_path\": output_path\n",
    "        }\n",
    "\n",
    "        print(f\"   âœ… Completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"   ğŸ“Š Portal URL: {result.get('studio_url', 'N/A')}\")\n",
    "        print(f\"   ğŸ’¾ Local results: {output_path}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error evaluating {model_name}: {str(e)}\\n\")\n",
    "        evaluation_results[model_name] = {\n",
    "            \"error\": str(e),\n",
    "            \"evaluation_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "print(\"âœ… All evaluations complete!\")\n",
    "print(f\"\\nğŸ“ All results saved to: {output_dir}/\")\n",
    "print(f\"ğŸŒ View results in Azure AI Foundry portal using the URLs above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7996f57",
   "metadata": {},
   "source": [
    "## Step 9: ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "ëª¨ë“  ëª¨ë¸ì— ëŒ€í•œ ì£¼ìš” ì§€í‘œ ë¹„êµ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df73aa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Eval Time (s)</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Fluency</th>\n",
       "      <th>Violence</th>\n",
       "      <th>Hate/Unfairness</th>\n",
       "      <th>Self-Harm</th>\n",
       "      <th>Sexual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>51.421137</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>50.138621</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>46.001300</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Eval Time (s)  Relevance  Coherence  Fluency  Violence  \\\n",
       "0  gpt-4o-mini      51.421137        4.6        4.2      3.6       0.0   \n",
       "1       gpt-4o      50.138621        4.2        4.0      3.8       0.0   \n",
       "2      gpt-4.1      46.001300        4.8        4.4      3.8       0.0   \n",
       "\n",
       "   Hate/Unfairness  Self-Harm  Sexual  \n",
       "0              0.0        0.0     0.0  \n",
       "1              0.0        0.0     0.0  \n",
       "2              0.0        0.0     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare data for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    if \"error\" in results:\n",
    "        print(f\"âš ï¸  {model_name}: Evaluation failed - {results['error']}\\n\")\n",
    "        continue\n",
    "\n",
    "    metrics = results[\"metrics\"]\n",
    "\n",
    "    # The metrics are stored with keys like \"relevance.relevance\", \"coherence.coherence\", etc.\n",
    "    row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Eval Time (s)\": results['evaluation_time'],\n",
    "        \"Relevance\": metrics.get('relevance.relevance', metrics.get('relevance', 0)),\n",
    "        \"Coherence\": metrics.get('coherence.coherence', metrics.get('coherence', 0)),\n",
    "        \"Fluency\": metrics.get('fluency.fluency', metrics.get('fluency', 0)),\n",
    "        \"Violence\": metrics.get('violence.violence_defect_rate', metrics.get('violence', 0)),\n",
    "        \"Hate/Unfairness\": metrics.get('hate_unfairness.hate_unfairness_defect_rate', metrics.get('hate_unfairness', 0)),\n",
    "        \"Self-Harm\": metrics.get('self_harm.self_harm_defect_rate', metrics.get('self_harm', 0)),\n",
    "        \"Sexual\": metrics.get('sexual.sexual_defect_rate', metrics.get('sexual', 0)),\n",
    "    }\n",
    "\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"ğŸ“Š Model Evaluation Comparison\\n\")\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b25e71",
   "metadata": {},
   "source": [
    "## Step 10: ì„±ëŠ¥ ìš”ì•½\n",
    "\n",
    "í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•´ ë‹¤ì–‘í•œ ì§€í‘œì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì‹ë³„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f3e881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ğŸ† Best Performing Models by Metric"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Best Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4.800</td>\n",
       "      <td>â†‘ Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coherence</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4.400</td>\n",
       "      <td>â†‘ Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fluency</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>3.800</td>\n",
       "      <td>â†‘ Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Violence Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>â†“ Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hate/Unfairness Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>â†“ Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Self-Harm Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>â†“ Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sexual Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>â†“ Lower is Better</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric   Best Model  Score           Direction\n",
       "0               Relevance      gpt-4.1  4.800  â†‘ Higher is Better\n",
       "1               Coherence      gpt-4.1  4.400  â†‘ Higher is Better\n",
       "2                 Fluency       gpt-4o  3.800  â†‘ Higher is Better\n",
       "3         Violence Safety  gpt-4o-mini  0.000   â†“ Lower is Better\n",
       "4  Hate/Unfairness Safety  gpt-4o-mini  0.000   â†“ Lower is Better\n",
       "5        Self-Harm Safety  gpt-4o-mini  0.000   â†“ Lower is Better\n",
       "6           Sexual Safety  gpt-4o-mini  0.000   â†“ Lower is Better"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ğŸŒŸ Overall Best Model (Quality Metrics Average)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ğŸ¥‡ **gpt-4.1** - Quality: 4.333 | Time: 46.00s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg Quality Score</th>\n",
       "      <th>Eval Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¥‡ 1</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4.333</td>\n",
       "      <td>46.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ¥ˆ 2</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>4.133</td>\n",
       "      <td>51.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ğŸ¥‰ 3</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>4.000</td>\n",
       "      <td>50.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank        Model Avg Quality Score Eval Time (s)\n",
       "0  ğŸ¥‡ 1      gpt-4.1             4.333         46.00\n",
       "1  ğŸ¥ˆ 2  gpt-4o-mini             4.133         51.42\n",
       "2  ğŸ¥‰ 3       gpt-4o             4.000         50.14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Check for successful evaluations\n",
    "successful_models = {name: results for name,\n",
    "                     results in evaluation_results.items() if 'error' not in results}\n",
    "failed_models = {name: results for name,\n",
    "                 results in evaluation_results.items() if 'error' in results}\n",
    "\n",
    "if successful_models:\n",
    "    display(Markdown(\"## ğŸ† Best Performing Models by Metric\"))\n",
    "\n",
    "    # Define evaluator metrics and their optimization direction\n",
    "    evaluator_metrics = [\n",
    "        ('relevance.relevance', 'Relevance', True),\n",
    "        ('coherence.coherence', 'Coherence', True),\n",
    "        ('fluency.fluency', 'Fluency', True),\n",
    "        ('violence.violence_defect_rate', 'Violence Safety', False),\n",
    "        ('hate_unfairness.hate_unfairness_defect_rate',\n",
    "         'Hate/Unfairness Safety', False),\n",
    "        ('self_harm.self_harm_defect_rate', 'Self-Harm Safety', False),\n",
    "        ('sexual.sexual_defect_rate', 'Sexual Safety', False),\n",
    "    ]\n",
    "\n",
    "    # Create a dataframe for best models\n",
    "    best_models_data = []\n",
    "\n",
    "    for metric_key, display_name, higher_is_better in evaluator_metrics:\n",
    "        valid_models = {}\n",
    "\n",
    "        # Collect scores for this metric from all successful models\n",
    "        for model_name, results in successful_models.items():\n",
    "            metrics = results['metrics']\n",
    "            score = metrics.get(metric_key)\n",
    "            if score is not None:\n",
    "                valid_models[model_name] = score\n",
    "\n",
    "        if valid_models:\n",
    "            # Find best model based on optimization direction\n",
    "            if higher_is_better:\n",
    "                best_model_name = max(valid_models, key=valid_models.get)\n",
    "                best_score = valid_models[best_model_name]\n",
    "                direction = \"â†‘ Higher is Better\"\n",
    "            else:\n",
    "                best_model_name = min(valid_models, key=valid_models.get)\n",
    "                best_score = valid_models[best_model_name]\n",
    "                direction = \"â†“ Lower is Better\"\n",
    "\n",
    "            best_models_data.append({\n",
    "                \"Metric\": display_name,\n",
    "                \"Best Model\": best_model_name,\n",
    "                \"Score\": f\"{best_score:.3f}\",\n",
    "                \"Direction\": direction\n",
    "            })\n",
    "\n",
    "    df_best = pd.DataFrame(best_models_data)\n",
    "    display(df_best)\n",
    "\n",
    "    # Calculate overall best model (based on quality metrics average)\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"## ğŸŒŸ Overall Best Model (Quality Metrics Average)\"))\n",
    "\n",
    "    quality_metric_keys = ['relevance.relevance',\n",
    "                           'coherence.coherence', 'fluency.fluency']\n",
    "    model_quality_scores = {}\n",
    "\n",
    "    for model_name, results in successful_models.items():\n",
    "        metrics = results['metrics']\n",
    "        scores = []\n",
    "        for metric_key in quality_metric_keys:\n",
    "            score = metrics.get(metric_key)\n",
    "            if score is not None:\n",
    "                scores.append(score)\n",
    "\n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            model_quality_scores[model_name] = {\n",
    "                'avg_quality': avg_score,\n",
    "                'eval_time': results['evaluation_time']\n",
    "            }\n",
    "\n",
    "    if model_quality_scores:\n",
    "        best_overall = max(model_quality_scores,\n",
    "                           key=lambda x: model_quality_scores[x]['avg_quality'])\n",
    "        best_data = model_quality_scores[best_overall]\n",
    "\n",
    "        display(Markdown(\n",
    "            f\"ğŸ¥‡ **{best_overall}** - Quality: {best_data['avg_quality']:.3f} | Time: {best_data['eval_time']:.2f}s\"))\n",
    "\n",
    "        # Show all model quality scores for comparison\n",
    "        display(Markdown(\"\"))\n",
    "\n",
    "        ranking_data = []\n",
    "        sorted_models = sorted(model_quality_scores.items(\n",
    "        ), key=lambda x: x[1]['avg_quality'], reverse=True)\n",
    "\n",
    "        for rank, (model_name, data) in enumerate(sorted_models, 1):\n",
    "            medal = \"ğŸ¥‡\" if rank == 1 else \"ğŸ¥ˆ\" if rank == 2 else \"ğŸ¥‰\" if rank == 3 else \"\"\n",
    "            ranking_data.append({\n",
    "                \"Rank\": f\"{medal} {rank}\",\n",
    "                \"Model\": model_name,\n",
    "                \"Avg Quality Score\": f\"{data['avg_quality']:.3f}\",\n",
    "                \"Eval Time (s)\": f\"{data['eval_time']:.2f}\"\n",
    "            })\n",
    "\n",
    "        df_ranking = pd.DataFrame(ranking_data)\n",
    "        display(df_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07a9ac",
   "metadata": {},
   "source": [
    "## Step 11: ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì—¬ëŸ¬ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤! ê³ ë ¤í•  ë‹¤ìŒ ë‹¨ê³„:\n",
    "\n",
    "### ğŸ“Š ê²°ê³¼ ì¡°íšŒ ìœ„ì¹˜\n",
    "\n",
    "- **Azure AI Foundry í¬í„¸**: ëŒ€í™”í˜• ì‹œê°í™”(ìƒì„¸ ì°¨íŠ¸ ë° ë¹„êµ)\n",
    "- **í¬í„¸ URL**: ê° í‰ê°€ì— ëŒ€í•´ studio URL ì œê³µ, íŒ€ ê³µìœ ì— ìœ ìš©\n",
    "- **ë¡œì»¬ íŒŒì¼**: ëª¨ë“  ê²°ê³¼ëŠ” `./22-evaluate-models-results/`ì— ì €ì¥ë¨\n",
    "- **ë²„ì „ ê´€ë¦¬**: JSON íŒŒì¼ì„ ì»¤ë°‹í•´ ì¬í˜„ì„±ê³¼ ì‹œê°„ ê²½ê³¼ ì¶”ì  ì—¬ë¶€ í™•ë³´\n",
    "\n",
    "### ğŸ† ëª¨ë¸ ë¦¬ë”ë³´ë“œ ì‚¬ìš©\n",
    "\n",
    "- **ë¦¬ë”ë³´ë“œ íƒìƒ‰**: [í’ˆì§ˆ, ì•ˆì „ì„±, ë¹„ìš©, ì„±ëŠ¥ë³„ ë¹„êµ](https://learn.microsoft.com/azure/ai-foundry/how-to/benchmark-model-in-catalog)\n",
    "- **íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„**: í’ˆì§ˆ vs ë¹„ìš©, í’ˆì§ˆ vs ì•ˆì „ì„± ì°¨íŠ¸ ë³´ê¸°\n",
    "- **ì‹œë‚˜ë¦¬ì˜¤ í•„í„°ë§**: Q&A, ì½”ë“œ ìƒì„±, ì¶”ë¡  ë“± ì‚¬ìš© ì‚¬ë¡€ì— ì í•©í•œ ëª¨ë¸ ì°¾ê¸°\n",
    "- **í¬í„¸ ì ‘ê·¼**: [Microsoft Foundry Model Catalog â†’ Browse Leaderboards](https://aka.ms/model-leaderboards)\n",
    "\n",
    "---\n",
    "\n",
    "**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ì´ì œ ì—¬ëŸ¬ ëª¨ë¸ì— ëŒ€í•œ í¬ê´„ì  í‰ê°€ ì§€í‘œë¥¼ í™•ë³´í•˜ì˜€ìŠµë‹ˆë‹¤.** ğŸ‰\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
