{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745e6fbf",
   "metadata": {},
   "source": [
    "# üõçÔ∏è | Cora-For-Zava: Model Selection \n",
    "\n",
    "Welcome! This notebook will walk you through evaluating multiple AI models using a standardized test dataset and the Azure AI Evaluation SDK.\n",
    "\n",
    "## üõí Our Zava Scenario\n",
    "\n",
    "**Cora** is a customer service chatbot for **Zava** - a fictitious retailer of home improvement goods for DIY enthusiasts. To ensure Cora provides the best customer experience, you need to select the right foundation model. With multiple Azure OpenAI models available (GPT-4o, GPT-4o-mini, GPT-4), you need to evaluate which model delivers the best balance of quality, safety, and performance for your retail use case.\n",
    "\n",
    "## üéØ What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- ‚úÖ Configured multiple Azure OpenAI models for comparison\n",
    "- ‚úÖ Loaded standardized test datasets for evaluation\n",
    "- ‚úÖ Run evaluations across models using built-in evaluators\n",
    "- ‚úÖ Analyzed performance metrics (quality, safety, latency)\n",
    "- ‚úÖ Compared model results to make informed selection decisions\n",
    "\n",
    "## üí° What You'll Learn\n",
    "\n",
    "- How to configure multiple models for evaluation\n",
    "- How to load test datasets for evaluation\n",
    "- How to run evaluations with built-in evaluators\n",
    "- How to analyze and compare model performance\n",
    "- How to use Azure AI Foundry model leaderboards\n",
    "\n",
    "> **Note**: This demonstrates pre-production evaluation, which is essential before deploying AI applications.\n",
    "\n",
    "Ready to compare models? Let's get started! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecf1c6",
   "metadata": {},
   "source": [
    "## Step 1: Verify Environment Variables\n",
    "\n",
    "The following environment variables should already be configured in your `.env` file from the earlier setup steps:\n",
    "\n",
    "- **AZURE_OPENAI_API_KEY**: Your Azure OpenAI API key\n",
    "  - Ï†úÍ±∞ÌïòÍ≥† SystemManagedIdentity ÏÇ¨Ïö© ÏòàÏ†ï\n",
    "- **AZURE_OPENAI_ENDPOINT**: Your Azure OpenAI service endpoint\n",
    "- **AZURE_OPENAI_API_VERSION**: The API version to use\n",
    "- **AZURE_SUBSCRIPTION_ID**: Your Azure subscription ID\n",
    "- **AZURE_RESOURCE_GROUP**: Your Azure resource group name\n",
    "- **AZURE_AI_PROJECT_NAME**: Your Azure AI Foundry project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aa6c36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All environment variables configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Use override=True to reload any changes made to .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Verify all required Azure service credentials are available\n",
    "required_vars = [\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_API_VERSION\",\n",
    "    \"AZURE_SUBSCRIPTION_ID\",\n",
    "    \"AZURE_RESOURCE_GROUP\",\n",
    "    \"AZURE_AI_PROJECT_NAME\",\n",
    "    \"AZURE_AI_FOUNDRY_NAME\"\n",
    "]\n",
    "\n",
    "missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    raise EnvironmentError(\n",
    "        f\"‚ùå Missing environment variables: {', '.join(missing_vars)}\")\n",
    "\n",
    "print(\"‚úÖ All environment variables configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e63ed",
   "metadata": {},
   "source": [
    "## Step 2: Define Models to Evaluate\n",
    "\n",
    "Configure the array of model deployment names you want to evaluate. You can add or remove models from this list based on what's deployed in your Azure OpenAI resource.\n",
    "\n",
    "> **Tip**: Use [Azure AI Foundry Model Leaderboards](https://learn.microsoft.com/azure/ai-foundry/how-to/benchmark-model-in-catalog) to compare models on quality, safety, cost, and performance before deploying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51526b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configured 3 models for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Define the models to evaluate\n",
    "# Add or remove model deployment names as needed\n",
    "models_to_evaluate = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1\"\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Configured {len(models_to_evaluate)} models for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12afa1f",
   "metadata": {},
   "source": [
    "## üí° Model Selection with Leaderboards\n",
    "\n",
    "Before or after running custom evaluations, you can use Azure AI Foundry Model Leaderboards to help select the best models:\n",
    "\n",
    "**How to Access Leaderboards:**\n",
    "1. Go to [Azure AI Foundry portal](https://ai.azure.com)\n",
    "2. Select **Model catalog** from the left pane\n",
    "3. Click **Browse leaderboards** in the Model leaderboards section\n",
    "\n",
    "**What You Can Compare:**\n",
    "- **Quality Leaderboard**: Models ranked by accuracy on reasoning, Q&A, coding, and math tasks\n",
    "- **Safety Leaderboard**: Models ranked by resistance to harmful content\n",
    "- **Cost Leaderboard**: Models ranked by cost-effectiveness\n",
    "- **Performance Leaderboard**: Models ranked by throughput and latency\n",
    "- **Trade-off Charts**: Quality vs. Cost, Quality vs. Safety, Quality vs. Throughput\n",
    "- **Scenario-Specific**: Find models best for your use case (chatbots, code generation, etc.)\n",
    "\n",
    "This can help you narrow down which models to include in your `models_to_evaluate` list!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87373185",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Dataset\n",
    "\n",
    "Load the evaluation dataset with test queries and expected responses. This will be used as the test input for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae061aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 5 test examples from 22-evaluate-models.jsonl\n",
      "\n",
      "üìä Test Dataset Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was United States found ?</td>\n",
       "      <td>1776</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of finish does the durable eggshell ...</td>\n",
       "      <td>The durable eggshell finish paint has a subtle...</td>\n",
       "      <td>The durable eggshell finish paint has a subtle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What product fits standard paint trays for qui...</td>\n",
       "      <td>Disposable plastic liners that fit standard pa...</td>\n",
       "      <td>The product that fits standard paint trays for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which paint is recommended for kitchens, bathr...</td>\n",
       "      <td>Washable semi-gloss interior paint for kitchen...</td>\n",
       "      <td>The washable semi-gloss interior paint is reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                     When was United States found ?   \n",
       "1                     What is the capital of France?   \n",
       "2  What type of finish does the durable eggshell ...   \n",
       "3  What product fits standard paint trays for qui...   \n",
       "4  Which paint is recommended for kitchens, bathr...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0                                               1776   \n",
       "1                                              Paris   \n",
       "2  The durable eggshell finish paint has a subtle...   \n",
       "3  Disposable plastic liners that fit standard pa...   \n",
       "4  Washable semi-gloss interior paint for kitchen...   \n",
       "\n",
       "                                            response  \n",
       "0                                               1600  \n",
       "1                                              Paris  \n",
       "2  The durable eggshell finish paint has a subtle...  \n",
       "3  The product that fits standard paint trays for...  \n",
       "4  The washable semi-gloss interior paint is reco...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the evaluation dataset\n",
    "dataset_path = \"22-evaluate-models.jsonl\"\n",
    "test_data = []\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(test_data)} test examples from {dataset_path}\\n\")\n",
    "\n",
    "# Display as DataFrame for easy viewing\n",
    "df_test_data = pd.DataFrame(test_data)\n",
    "print(\"üìä Test Dataset Preview:\")\n",
    "display(df_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09901a6b",
   "metadata": {},
   "source": [
    "## Step 4: Configure Azure AI Project\n",
    "\n",
    "Set up the Azure AI project connection for running evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fb3075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure credentials verified successfully!\n",
      "‚úÖ Azure AI Project configured\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Create Azure AI project configuration\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group_name = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "\n",
    "# Dynamically construct the Azure AI Foundry project URL\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "# Initialize and verify credential\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Try to get a token to verify authentication\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    print(\"‚úÖ Azure credentials verified successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Azure credentials not found or expired!\")\n",
    "    print(\"Please run 'az login' in the terminal to authenticate with Azure.\")\n",
    "    raise\n",
    "\n",
    "print(f\"‚úÖ Azure AI Project configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f11c7",
   "metadata": {},
   "source": [
    "## Step 5: Create Model Configurations\n",
    "\n",
    "Create model configuration objects for each model we want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3addb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created configurations for 3 models\n"
     ]
    }
   ],
   "source": [
    "# Create model configurations for all models\n",
    "model_configs = {}\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    model_configs[model_name] = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=model_name,\n",
    "        credential=DefaultAzureCredential(),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Created configurations for {len(model_configs)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3af989",
   "metadata": {},
   "source": [
    "## Step 6: Define Target Function\n",
    "\n",
    "Create a function that takes a query and returns a response from a specific model. This will be used by the evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e23fa929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Target function factory created\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "\n",
    "def create_target_function(model_name):\n",
    "    \"\"\"Create a target function for a specific model\"\"\"\n",
    "\n",
    "    def target_function(query: str, ground_truth: str = None, response: str = None) -> dict:\n",
    "        \"\"\"Generate response from the model\"\"\"\n",
    "        client = AzureOpenAI(\n",
    "            azure_ad_token_provider=get_bearer_token_provider(\n",
    "                DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n",
    "            api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "\n",
    "        # Call the model with the query\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions accurately and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"response\": response.choices[0].message.content\n",
    "        }\n",
    "\n",
    "    return target_function\n",
    "\n",
    "\n",
    "print(\"‚úÖ Target function factory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10314e6e",
   "metadata": {},
   "source": [
    "## Step 7: Configure Evaluators\n",
    "\n",
    "Set up the evaluators we'll use to assess model performance. We'll use [built-in evaluators](https://learn.microsoft.com/azure/ai-foundry/concepts/observability#what-are-evaluators) for quality and safety metrics.\n",
    "\n",
    "**Quality Evaluators** (AI-assisted):\n",
    "- **Relevance**: Evaluates how pertinent responses are to the given questions (scale 1-5)\n",
    "- **Coherence**: Evaluates how well the output flows smoothly and reads naturally (scale 1-5)\n",
    "- **Fluency**: Evaluates language proficiency and grammatical correctness (scale 1-5)\n",
    "\n",
    "**Safety Evaluators** (Content safety):\n",
    "- **Violence**: Detects violent content in responses (scale 0-7, lower is safer)\n",
    "- **Hate/Unfairness**: Detects hateful or unfair content (scale 0-7, lower is safer)\n",
    "- **Self-Harm**: Detects self-harm related content (scale 0-7, lower is safer)\n",
    "- **Sexual**: Detects sexual content (scale 0-7, lower is safer)\n",
    "\n",
    "> **Note**: We're using Relevance, Coherence, and Fluency evaluators which don't require context or ground truth. Groundedness evaluator has been removed as it requires additional context that our simple dataset doesn't provide.\n",
    "\n",
    "> Learn more about [evaluation metrics](https://learn.microsoft.com/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics) and their use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388785fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluators configured:\n",
      "   Quality: Groundedness, Relevance, Coherence, Fluency\n",
      "   Safety: Violence, Hate/Unfairness, Self-Harm, Sexual\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    ViolenceEvaluator,\n",
    "    HateUnfairnessEvaluator,\n",
    "    SelfHarmEvaluator,\n",
    "    SexualEvaluator\n",
    ")\n",
    "\n",
    "# Create a judge model configuration for evaluators\n",
    "judge_model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=\"gpt-4o\",  # Use a capable model as judge\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# Initialize quality evaluators\n",
    "groundedness_eval = GroundednessEvaluator(model_config=judge_model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config=judge_model_config)\n",
    "coherence_eval = CoherenceEvaluator(model_config=judge_model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config=judge_model_config)\n",
    "\n",
    "# Initialize safety evaluators (using azure_ai_project_url instead of dictionary)\n",
    "violence_eval = ViolenceEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "hate_unfairness_eval = HateUnfairnessEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "self_harm_eval = SelfHarmEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "sexual_eval = SexualEvaluator(\n",
    "    azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "\n",
    "print(\"‚úÖ Evaluators configured:\")\n",
    "print(\"   Quality: Groundedness, Relevance, Coherence, Fluency\")\n",
    "print(\"   Safety: Violence, Hate/Unfairness, Self-Harm, Sexual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "688f8077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running configuration test...\n",
      "============================================================\n",
      "\n",
      "üìã Test Model: gpt-4o-mini\n",
      "üìù Test Query: What is the capital of France?\n",
      "\n",
      "1Ô∏è‚É£ Testing target function...\n",
      "   ‚úÖ Target function returned: The capital of France is Paris....\n",
      "\n",
      "2Ô∏è‚É£ Testing evaluation pipeline with portal publishing...\n",
      "2025-12-20 05:40:46 +0000 281472351105488 execution.bulk     INFO     Finished 1 / 1 lines.\n",
      "2025-12-20 05:40:46 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 2.01 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"create_target_function__locals__target_function_20251220_054044_449106\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:40:44.449106+00:00\"\n",
      "Duration: \"0:00:03.012534\"\n",
      "\n",
      "2025-12-20 05:40:50 +0000 281472342712784 execution.bulk     INFO     Finished 1 / 1 lines.\n",
      "2025-12-20 05:40:50 +0000 281472342712784 execution.bulk     INFO     Average execution time for completed lines: 2.54 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20251220_054047_484836\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:40:47.484836+00:00\"\n",
      "Duration: \"0:00:03.009024\"\n",
      "\n",
      "2025-12-20 05:40:50 +0000 281472351105488 execution.bulk     INFO     Finished 1 / 1 lines.\n",
      "2025-12-20 05:40:50 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 3.43 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"coherence_20251220_054047_479201\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:40:47.479201+00:00\"\n",
      "Duration: \"0:00:04.011989\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.009024\",\n",
      "        \"completed_lines\": 1,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"coherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:04.011989\",\n",
      "        \"completed_lines\": 1,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "   ‚úÖ Evaluation completed successfully!\n",
      "   üìä Sample metrics:\n",
      "      - Relevance: N/A\n",
      "      - Coherence: N/A\n",
      "\n",
      "   üåê View test results in portal:\n",
      "      https://ai.azure.com/resource/build/evaluation/9ccb78ea-1390-4ed5-ab1b-4009aae891ce?wsid=/subscriptions/0ce67698-ac36-4c1c-8188-e8336e25f023/resourceGroups/rg-Ignite-PREL13/providers/Microsoft.CognitiveServices/accounts/aoai-ob7apaqovowv2/projects/proj-ob7apaqovowv2&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "\n",
      "============================================================\n",
      "‚úÖ Configuration test PASSED! Ready to run full evaluation.\n",
      "============================================================\n",
      "\n",
      "üßπ Temporary test file cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Test with a single model and single prompt\n",
    "import tempfile\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üß™ Running configuration test...\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "# Select first model for testing\n",
    "test_model = models_to_evaluate[0]\n",
    "print(f\"\\nüìã Test Model: {test_model}\", flush=True)\n",
    "\n",
    "# Create a simple test dataset with one example\n",
    "test_example = {\n",
    "    \"query\": \"What is the capital of France?\",\n",
    "    \"ground_truth\": \"Paris\",\n",
    "    \"response\": \"Paris\"\n",
    "}\n",
    "\n",
    "# Save test example to a temporary file\n",
    "test_file = tempfile.NamedTemporaryFile(\n",
    "    mode='w', suffix='.jsonl', delete=False)\n",
    "test_file.write(json.dumps(test_example) + '\\n')\n",
    "test_file.close()\n",
    "\n",
    "print(f\"üìù Test Query: {test_example['query']}\", flush=True)\n",
    "\n",
    "try:\n",
    "    # Create target function for test model\n",
    "    test_target_fn = create_target_function(test_model)\n",
    "\n",
    "    # Test the target function\n",
    "    print(\"\\n1Ô∏è‚É£ Testing target function...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    test_result = test_target_fn(**test_example)\n",
    "    print(\n",
    "        f\"   ‚úÖ Target function returned: {test_result['response'][:100]}...\", flush=True)\n",
    "\n",
    "    # Test evaluation with minimal evaluators AND portal publishing\n",
    "    print(\"\\n2Ô∏è‚É£ Testing evaluation pipeline with portal publishing...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    from azure.ai.evaluation import evaluate\n",
    "\n",
    "    test_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    test_eval_result = evaluate(\n",
    "        data=test_file.name,\n",
    "        target=test_target_fn,\n",
    "        evaluators={\n",
    "            \"relevance\": relevance_eval,\n",
    "            \"coherence\": coherence_eval,\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"default\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${target.response}\",\n",
    "            }\n",
    "        },\n",
    "        # Publish to portal for verification (using URL format)\n",
    "        azure_ai_project=azure_ai_project_url,\n",
    "        evaluation_name=f\"22-evaluate-models-TEST_{test_model}_{test_timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n   ‚úÖ Evaluation completed successfully!\", flush=True)\n",
    "    print(f\"   üìä Sample metrics:\", flush=True)\n",
    "    print(\n",
    "        f\"      - Relevance: {test_eval_result['metrics'].get('relevance', 'N/A')}\", flush=True)\n",
    "    print(\n",
    "        f\"      - Coherence: {test_eval_result['metrics'].get('coherence', 'N/A')}\", flush=True)\n",
    "\n",
    "    # Show portal URL if available\n",
    "    if test_eval_result.get('studio_url'):\n",
    "        print(f\"\\n   üåê View test results in portal:\", flush=True)\n",
    "        print(f\"      {test_eval_result['studio_url']}\", flush=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60, flush=True)\n",
    "    print(\"‚úÖ Configuration test PASSED! Ready to run full evaluation.\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Configuration test FAILED!\", flush=True)\n",
    "    print(f\"Error: {str(e)}\", flush=True)\n",
    "    print(\"\\nPlease fix the configuration before proceeding to Step 8.\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    sys.stdout.flush()\n",
    "    raise\n",
    "finally:\n",
    "    # Clean up temporary file\n",
    "    import os\n",
    "    if os.path.exists(test_file.name):\n",
    "        os.unlink(test_file.name)\n",
    "    print(\"\\nüßπ Temporary test file cleaned up.\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3efc5",
   "metadata": {},
   "source": [
    "## Step 8: Run Evaluations\n",
    "\n",
    "Now we'll evaluate each model using the test dataset and the [`evaluate()` function](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk#local-evaluation-on-test-datasets-using-evaluate). This will generate comprehensive metrics for performance, quality, and safety.\n",
    "\n",
    "Each evaluation:\n",
    "- Tests the model with all examples from the dataset\n",
    "- Calculates quality metrics using a judge model\n",
    "- Assesses safety using Azure AI Content Safety\n",
    "- Tracks evaluation time (as a proxy for latency)\n",
    "- **Publishes results to Azure AI Foundry portal** for visualization\n",
    "- **Saves detailed results locally** for offline analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa09801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting evaluation of 3 models...\n",
      "   Test dataset size: 5 examples\n",
      "   Output directory: ./22-evaluate-models-results\n",
      "\n",
      "üìä Evaluating model: gpt-4o-mini\n",
      "2025-12-20 05:41:07 +0000 281472317534672 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:41:07 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 3.39 seconds. Estimated time for incomplete lines: 13.56 seconds.\n",
      "2025-12-20 05:41:07 +0000 281472317534672 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:41:07 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 1.75 seconds. Estimated time for incomplete lines: 5.25 seconds.\n",
      "2025-12-20 05:41:08 +0000 281472317534672 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:41:08 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 0.96 seconds.\n",
      "2025-12-20 05:41:08 +0000 281472317534672 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:41:08 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 0.79 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"create_target_function__locals__target_function_20251220_054104_318404\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:41:04.318404+00:00\"\n",
      "Duration: \"0:00:04.017270\"\n",
      "\n",
      "2025-12-20 05:41:14 +0000 281472334320080 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 35.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:41:14 +0000 281472334320080 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 35.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:41:14 +0000 281472334320080 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 35.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:41:14 +0000 281472334320080 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 35.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:41:14 +0000 281472334320080 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 35.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 35 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:41:14 +0000 281472325927376 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:41:14 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 6.56 seconds. Estimated time for incomplete lines: 26.24 seconds.\n",
      "2025-12-20 05:41:15 +0000 281472325927376 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 3.43 seconds. Estimated time for incomplete lines: 10.29 seconds.\n",
      "2025-12-20 05:41:15 +0000 281472325927376 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 2.32 seconds. Estimated time for incomplete lines: 4.64 seconds.\n",
      "2025-12-20 05:41:15 +0000 281472325927376 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 1.74 seconds. Estimated time for incomplete lines: 1.74 seconds.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 6.99 seconds. Estimated time for incomplete lines: 27.96 seconds.\n",
      "2025-12-20 05:41:15 +0000 281472325927376 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 1.4 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20251220_054108_355224\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:41:08.355224+00:00\"\n",
      "Duration: \"0:00:07.204553\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 3.64 seconds. Estimated time for incomplete lines: 10.92 seconds.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 2.47 seconds. Estimated time for incomplete lines: 4.94 seconds.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 1.86 seconds. Estimated time for incomplete lines: 1.86 seconds.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:41:15 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 1.49 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"coherence_20251220_054108_351691\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:41:08.351691+00:00\"\n",
      "Duration: \"0:00:07.977608\"\n",
      "\n",
      "2025-12-20 05:41:24 +0000 281472309141968 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:41:24 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 8.28 seconds. Estimated time for incomplete lines: 24.84 seconds.\n",
      "2025-12-20 05:41:25 +0000 281471956808144 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:41:25 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 16.9 seconds. Estimated time for incomplete lines: 67.6 seconds.\n",
      "2025-12-20 05:41:25 +0000 281471948415440 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:41:25 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 5.67 seconds. Estimated time for incomplete lines: 11.34 seconds.\n",
      "2025-12-20 05:41:25 +0000 281472317534672 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:41:25 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 5.7 seconds. Estimated time for incomplete lines: 11.4 seconds.\n",
      "2025-12-20 05:41:28 +0000 281471956808144 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:41:28 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 10.28 seconds. Estimated time for incomplete lines: 30.84 seconds.\n",
      "2025-12-20 05:41:29 +0000 281471956808144 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:41:29 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 6.98 seconds. Estimated time for incomplete lines: 13.96 seconds.\n",
      "2025-12-20 05:41:29 +0000 281471956808144 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:41:29 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 5.34 seconds. Estimated time for incomplete lines: 5.34 seconds.\n",
      "2025-12-20 05:41:30 +0000 281472309141968 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:41:30 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 7.3 seconds. Estimated time for incomplete lines: 14.6 seconds.\n",
      "2025-12-20 05:41:37 +0000 281472309141968 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:41:37 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 7.22 seconds. Estimated time for incomplete lines: 7.22 seconds.\n",
      "2025-12-20 05:41:37 +0000 281472309141968 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:41:37 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 5.84 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"hate_unfairness_20251220_054108_362897\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:41:08.362897+00:00\"\n",
      "Duration: \"0:00:29.196249\"\n",
      "\n",
      "2025-12-20 05:41:37 +0000 281471948415440 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:41:37 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 7.4 seconds. Estimated time for incomplete lines: 7.4 seconds.\n",
      "2025-12-20 05:41:38 +0000 281472317534672 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:41:38 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 7.42 seconds. Estimated time for incomplete lines: 7.42 seconds.\n",
      "2025-12-20 05:41:38 +0000 281471956808144 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:41:38 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 6.1 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"violence_20251220_054108_364243\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:41:08.364243+00:00\"\n",
      "Duration: \"0:00:30.516284\"\n",
      "\n",
      "2025-12-20 05:41:38 +0000 281471948415440 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:41:38 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 6.1 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"self_harm_20251220_054108_374672\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:41:08.374672+00:00\"\n",
      "Duration: \"0:00:30.626240\"\n",
      "\n",
      "2025-12-20 05:41:39 +0000 281472317534672 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:41:39 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 6.13 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"sexual_20251220_054108_366920\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:41:08.366920+00:00\"\n",
      "Duration: \"0:00:30.688754\"\n",
      "\n",
      "2025-12-20 05:41:54 +0000 281472334320080 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:41:54 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 46.37 seconds. Estimated time for incomplete lines: 185.48 seconds.\n",
      "2025-12-20 05:41:55 +0000 281472334320080 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:41:55 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 23.36 seconds. Estimated time for incomplete lines: 70.08 seconds.\n",
      "2025-12-20 05:41:55 +0000 281472334320080 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:41:55 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 11.77 seconds. Estimated time for incomplete lines: 11.77 seconds.\n",
      "2025-12-20 05:41:55 +0000 281472334320080 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:41:55 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 9.46 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"fluency_20251220_054108_360263\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:41:08.360263+00:00\"\n",
      "Duration: \"0:00:47.552339\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:07.204553\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"coherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:07.977608\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"fluency\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:47.552339\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"violence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:30.516284\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"hate_unfairness\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:29.196249\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"self_harm\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:30.626240\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"sexual\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:30.688754\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "Evaluation results saved to \"/workspaces/ignite25-PREL13-observe-manage-and-scale-agentic-ai-apps-with-microsoft-foundry/labs/2-models/22-evaluate-models-results/22-evaluate-models_gpt-4o-mini_20251220_054104\".\n",
      "\n",
      "   ‚úÖ Completed in 58.09 seconds\n",
      "   üìä Portal URL: https://ai.azure.com/resource/build/evaluation/8467882c-b44b-4b70-a520-49c46ab673bc?wsid=/subscriptions/0ce67698-ac36-4c1c-8188-e8336e25f023/resourceGroups/rg-Ignite-PREL13/providers/Microsoft.CognitiveServices/accounts/aoai-ob7apaqovowv2/projects/proj-ob7apaqovowv2&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "   üíæ Local results: ./22-evaluate-models-results/22-evaluate-models_gpt-4o-mini_20251220_054104\n",
      "\n",
      "üìä Evaluating model: gpt-4o\n",
      "2025-12-20 05:42:15 +0000 281472317534672 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:42:15 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 13.52 seconds. Estimated time for incomplete lines: 54.08 seconds.\n",
      "2025-12-20 05:42:16 +0000 281472317534672 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:42:16 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 6.85 seconds. Estimated time for incomplete lines: 20.55 seconds.\n",
      "2025-12-20 05:42:16 +0000 281472317534672 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:42:16 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 4.68 seconds. Estimated time for incomplete lines: 9.36 seconds.\n",
      "2025-12-20 05:42:16 +0000 281472317534672 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:42:16 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 3.52 seconds. Estimated time for incomplete lines: 3.52 seconds.\n",
      "2025-12-20 05:42:16 +0000 281472317534672 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:42:16 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 2.85 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"create_target_function__locals__target_function_20251220_054202_414424\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:42:02.414424+00:00\"\n",
      "Duration: \"0:00:15.068578\"\n",
      "\n",
      "2025-12-20 05:42:22 +0000 281472309141968 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:42:22 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 5.07 seconds. Estimated time for incomplete lines: 20.28 seconds.\n",
      "2025-12-20 05:42:22 +0000 281472309141968 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:42:22 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 2.61 seconds. Estimated time for incomplete lines: 7.83 seconds.\n",
      "2025-12-20 05:42:22 +0000 281472309141968 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:42:22 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 1.76 seconds. Estimated time for incomplete lines: 3.52 seconds.\n",
      "2025-12-20 05:42:22 +0000 281472309141968 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:42:22 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 1.35 seconds. Estimated time for incomplete lines: 1.35 seconds.\n",
      "2025-12-20 05:42:23 +0000 281472325927376 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:42:23 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 5.94 seconds. Estimated time for incomplete lines: 23.76 seconds.\n",
      "2025-12-20 05:42:23 +0000 281471956808144 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:42:23 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 6.03 seconds. Estimated time for incomplete lines: 24.12 seconds.\n",
      "2025-12-20 05:42:23 +0000 281472325927376 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:42:23 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 3.1 seconds. Estimated time for incomplete lines: 9.3 seconds.\n",
      "2025-12-20 05:42:23 +0000 281471956808144 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:42:23 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 3.14 seconds. Estimated time for incomplete lines: 9.42 seconds.\n",
      "2025-12-20 05:42:23 +0000 281472325927376 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:42:23 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 2.1 seconds. Estimated time for incomplete lines: 4.2 seconds.\n",
      "2025-12-20 05:42:23 +0000 281471956808144 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:42:23 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 2.1 seconds. Estimated time for incomplete lines: 4.2 seconds.\n",
      "2025-12-20 05:42:23 +0000 281472309141968 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:42:23 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 1.27 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-12-20 05:42:23 +0000 281472325927376 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:42:23 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 1.62 seconds. Estimated time for incomplete lines: 1.62 seconds.\n",
      "2025-12-20 05:42:24 +0000 281471956808144 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:42:24 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 1.65 seconds. Estimated time for incomplete lines: 1.65 seconds.\n",
      "2025-12-20 05:42:24 +0000 281472325927376 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:42:24 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 1.34 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-12-20 05:42:24 +0000 281471956808144 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:42:24 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 1.37 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"fluency_20251220_054217_500265\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:42:17.500265+00:00\"\n",
      "Duration: \"0:00:07.292067\"\n",
      "\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"coherence_20251220_054217_503853\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:42:17.503853+00:00\"\n",
      "Duration: \"0:00:07.301405\"\n",
      "\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20251220_054217_496431\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:42:17.496431+00:00\"\n",
      "Duration: \"0:00:07.317705\"\n",
      "\n",
      "2025-12-20 05:42:33 +0000 281471948415440 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:42:33 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 7.78 seconds. Estimated time for incomplete lines: 23.34 seconds.\n",
      "2025-12-20 05:42:33 +0000 281472317534672 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:42:33 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 8.02 seconds. Estimated time for incomplete lines: 24.06 seconds.\n",
      "2025-12-20 05:42:33 +0000 281472351105488 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:42:33 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 8.04 seconds. Estimated time for incomplete lines: 24.12 seconds.\n",
      "2025-12-20 05:42:33 +0000 281472334320080 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:42:33 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 8.09 seconds. Estimated time for incomplete lines: 24.27 seconds.\n",
      "2025-12-20 05:42:36 +0000 281471948415440 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:42:36 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 6.49 seconds. Estimated time for incomplete lines: 12.98 seconds.\n",
      "2025-12-20 05:42:37 +0000 281472334320080 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:42:37 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 6.79 seconds. Estimated time for incomplete lines: 13.58 seconds.\n",
      "2025-12-20 05:42:38 +0000 281472317534672 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:42:38 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 6.84 seconds. Estimated time for incomplete lines: 13.68 seconds.\n",
      "2025-12-20 05:42:39 +0000 281472334320080 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:42:39 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 5.43 seconds. Estimated time for incomplete lines: 5.43 seconds.\n",
      "2025-12-20 05:42:40 +0000 281471948415440 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:42:40 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 5.81 seconds. Estimated time for incomplete lines: 5.81 seconds.\n",
      "2025-12-20 05:42:45 +0000 281472351105488 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:42:45 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 9.49 seconds. Estimated time for incomplete lines: 18.98 seconds.\n",
      "2025-12-20 05:42:45 +0000 281472317534672 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:42:45 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 7.12 seconds. Estimated time for incomplete lines: 7.12 seconds.\n",
      "2025-12-20 05:42:48 +0000 281471948415440 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:42:48 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 6.28 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"hate_unfairness_20251220_054217_508338\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:42:17.508338+00:00\"\n",
      "Duration: \"0:00:31.395667\"\n",
      "\n",
      "2025-12-20 05:43:02 +0000 281472351105488 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:02 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 11.28 seconds. Estimated time for incomplete lines: 11.28 seconds.\n",
      "2025-12-20 05:43:02 +0000 281472334320080 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:02 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 9.06 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"self_harm_20251220_054217_511887\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:42:17.511887+00:00\"\n",
      "Duration: \"0:00:45.281759\"\n",
      "\n",
      "2025-12-20 05:43:03 +0000 281472317534672 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:03 +0000 281472351105488 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:03 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 9.25 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"sexual_20251220_054217_518212\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:42:17.518212+00:00\"\n",
      "Duration: \"0:00:46.270569\"\n",
      "\n",
      "2025-12-20 05:43:03 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 9.26 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"violence_20251220_054217_507292\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:42:17.507292+00:00\"\n",
      "Duration: \"0:00:46.282727\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:07.317705\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"coherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:07.301405\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"fluency\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:07.292067\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"violence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:46.282727\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"hate_unfairness\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:31.395667\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"self_harm\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:45.281759\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"sexual\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:46.270569\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "Evaluation results saved to \"/workspaces/ignite25-PREL13-observe-manage-and-scale-agentic-ai-apps-with-microsoft-foundry/labs/2-models/22-evaluate-models-results/22-evaluate-models_gpt-4o_20251220_054202\".\n",
      "\n",
      "   ‚úÖ Completed in 67.85 seconds\n",
      "   üìä Portal URL: https://ai.azure.com/resource/build/evaluation/e4e4367b-e1e4-4a28-90f0-e90c750901fc?wsid=/subscriptions/0ce67698-ac36-4c1c-8188-e8336e25f023/resourceGroups/rg-Ignite-PREL13/providers/Microsoft.CognitiveServices/accounts/aoai-ob7apaqovowv2/projects/proj-ob7apaqovowv2&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "   üíæ Local results: ./22-evaluate-models-results/22-evaluate-models_gpt-4o_20251220_054202\n",
      "\n",
      "üìä Evaluating model: gpt-4.1\n",
      "2025-12-20 05:43:12 +0000 281472317534672 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:43:12 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 2.64 seconds. Estimated time for incomplete lines: 10.56 seconds.\n",
      "2025-12-20 05:43:13 +0000 281472317534672 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:43:13 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 1.38 seconds. Estimated time for incomplete lines: 4.14 seconds.\n",
      "2025-12-20 05:43:13 +0000 281472317534672 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:43:13 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 0.93 seconds. Estimated time for incomplete lines: 1.86 seconds.\n",
      "2025-12-20 05:43:13 +0000 281472317534672 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:13 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 0.7 seconds. Estimated time for incomplete lines: 0.7 seconds.\n",
      "2025-12-20 05:43:13 +0000 281472317534672 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:13 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 0.57 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"create_target_function__locals__target_function_20251220_054310_269757\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:43:10.269757+00:00\"\n",
      "Duration: \"0:00:03.011517\"\n",
      "\n",
      "2025-12-20 05:43:17 +0000 281471956808144 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281471956808144 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281471956808144 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281471956808144 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281471956808144 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472309141968 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472309141968 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472309141968 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472309141968 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472309141968 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:17 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [0/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 5.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the token rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 5 seconds. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:26 +0000 281472309141968 azure.ai.evaluation._legacy.prompty._prompty WARNING  [1/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 1.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:26 +0000 281472309141968 azure.ai.evaluation._legacy.prompty._prompty WARNING  [1/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 1.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:26 +0000 281472309141968 azure.ai.evaluation._legacy.prompty._prompty WARNING  [1/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 1.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:26 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [1/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 1.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:26 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [1/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 1.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:26 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [1/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 1.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:26 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [1/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 1.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:26 +0000 281472351105488 azure.ai.evaluation._legacy.prompty._prompty WARNING  [1/10] AsyncAzureOpenAI request failed. RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}. Retrying in 1.000000 seconds.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/prompty/_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/azure/ai/evaluation/_legacy/_batch_engine/_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2603, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Your requests to gpt-4o for gpt-4o in Sweden Central have exceeded the call rate limit for your current AIServices S0 pricing tier. This request was for ChatCompletions_Create under Azure OpenAI API version 2025-02-01-preview. Please retry after 1 second. To increase your default rate limit, visit: https://aka.ms/oai/quotaincrease.'}}\n",
      "2025-12-20 05:43:29 +0000 281471956808144 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:43:29 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 16.2 seconds. Estimated time for incomplete lines: 64.8 seconds.\n",
      "2025-12-20 05:43:29 +0000 281471956808144 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:43:29 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 8.12 seconds. Estimated time for incomplete lines: 24.36 seconds.\n",
      "2025-12-20 05:43:29 +0000 281472334320080 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:43:29 +0000 281472325927376 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:43:29 +0000 281472317534672 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:29 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 4.12 seconds. Estimated time for incomplete lines: 4.12 seconds.\n",
      "2025-12-20 05:43:29 +0000 281471956808144 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:43:29 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 8.24 seconds. Estimated time for incomplete lines: 24.72 seconds.\n",
      "2025-12-20 05:43:29 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 8.23 seconds. Estimated time for incomplete lines: 24.69 seconds.\n",
      "2025-12-20 05:43:29 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 5.49 seconds. Estimated time for incomplete lines: 10.98 seconds.\n",
      "2025-12-20 05:43:29 +0000 281471956808144 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:29 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 4.16 seconds. Estimated time for incomplete lines: 4.16 seconds.\n",
      "2025-12-20 05:43:30 +0000 281471948415440 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:43:30 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 8.41 seconds. Estimated time for incomplete lines: 25.23 seconds.\n",
      "2025-12-20 05:43:30 +0000 281471956808144 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:30 +0000 281471956808144 execution.bulk     INFO     Average execution time for completed lines: 3.37 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"coherence_20251220_054313_300301\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:43:13.300301+00:00\"\n",
      "Duration: \"0:00:17.260616\"\n",
      "\n",
      "2025-12-20 05:43:30 +0000 281472309141968 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:43:30 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 17.36 seconds. Estimated time for incomplete lines: 69.44 seconds.\n",
      "2025-12-20 05:43:31 +0000 281472309141968 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:43:31 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 8.86 seconds. Estimated time for incomplete lines: 26.58 seconds.\n",
      "2025-12-20 05:43:31 +0000 281472351105488 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-12-20 05:43:31 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 18.61 seconds. Estimated time for incomplete lines: 74.44 seconds.\n",
      "2025-12-20 05:43:32 +0000 281472351105488 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-12-20 05:43:32 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 9.38 seconds. Estimated time for incomplete lines: 28.14 seconds.\n",
      "2025-12-20 05:43:32 +0000 281472351105488 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:43:32 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 6.26 seconds. Estimated time for incomplete lines: 12.52 seconds.\n",
      "2025-12-20 05:43:32 +0000 281472351105488 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:32 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 4.78 seconds. Estimated time for incomplete lines: 4.78 seconds.\n",
      "2025-12-20 05:43:32 +0000 281472309141968 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:43:32 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 6.46 seconds. Estimated time for incomplete lines: 12.92 seconds.\n",
      "2025-12-20 05:43:32 +0000 281472309141968 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:32 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 4.88 seconds. Estimated time for incomplete lines: 4.88 seconds.\n",
      "2025-12-20 05:43:32 +0000 281472351105488 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:32 +0000 281472351105488 execution.bulk     INFO     Average execution time for completed lines: 3.9 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20251220_054313_306126\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:43:13.306126+00:00\"\n",
      "Duration: \"0:00:20.119187\"\n",
      "\n",
      "2025-12-20 05:43:33 +0000 281472334320080 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:43:33 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 6.79 seconds. Estimated time for incomplete lines: 13.58 seconds.\n",
      "2025-12-20 05:43:33 +0000 281472325927376 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:43:33 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 6.82 seconds. Estimated time for incomplete lines: 13.64 seconds.\n",
      "2025-12-20 05:43:34 +0000 281472309141968 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:34 +0000 281472309141968 execution.bulk     INFO     Average execution time for completed lines: 4.15 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-12-20 05:43:34 +0000 281472334320080 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:34 +0000 281471948415440 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-12-20 05:43:34 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 5.19 seconds. Estimated time for incomplete lines: 5.19 seconds.\n",
      "2025-12-20 05:43:34 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 6.92 seconds. Estimated time for incomplete lines: 13.84 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"fluency_20251220_054313_293883\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:43:13.293883+00:00\"\n",
      "Duration: \"0:00:21.397616\"\n",
      "\n",
      "2025-12-20 05:43:35 +0000 281472317534672 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:35 +0000 281472317534672 execution.bulk     INFO     Average execution time for completed lines: 4.4 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"sexual_20251220_054313_309185\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:43:13.309185+00:00\"\n",
      "Duration: \"0:00:21.986884\"\n",
      "\n",
      "2025-12-20 05:43:42 +0000 281472325927376 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:42 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 7.27 seconds. Estimated time for incomplete lines: 7.27 seconds.\n",
      "2025-12-20 05:43:42 +0000 281471948415440 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-12-20 05:43:42 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 7.36 seconds. Estimated time for incomplete lines: 7.36 seconds.\n",
      "2025-12-20 05:43:43 +0000 281472334320080 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:43 +0000 281472325927376 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:43:43 +0000 281472334320080 execution.bulk     INFO     Average execution time for completed lines: 6.06 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"violence_20251220_054313_304643\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:43:13.304643+00:00\"\n",
      "Duration: \"0:00:30.303418\"\n",
      "\n",
      "2025-12-20 05:43:43 +0000 281472325927376 execution.bulk     INFO     Average execution time for completed lines: 6.06 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"hate_unfairness_20251220_054313_303688\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:43:13.303688+00:00\"\n",
      "Duration: \"0:00:30.306841\"\n",
      "\n",
      "2025-12-20 05:44:00 +0000 281471948415440 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-12-20 05:44:00 +0000 281471948415440 execution.bulk     INFO     Average execution time for completed lines: 9.36 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"self_harm_20251220_054313_310032\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-12-20 05:43:13.310032+00:00\"\n",
      "Duration: \"0:00:46.811249\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:20.119187\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"coherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:17.260616\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"fluency\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:21.397616\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"violence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:30.303418\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"hate_unfairness\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:30.306841\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"self_harm\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:46.811249\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"sexual\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:21.986884\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "Evaluation results saved to \"/workspaces/ignite25-PREL13-observe-manage-and-scale-agentic-ai-apps-with-microsoft-foundry/labs/2-models/22-evaluate-models-results/22-evaluate-models_gpt-4.1_20251220_054310\".\n",
      "\n",
      "   ‚úÖ Completed in 56.82 seconds\n",
      "   üìä Portal URL: https://ai.azure.com/resource/build/evaluation/17b9baa0-a1db-45b7-b057-42a6d5ce5543?wsid=/subscriptions/0ce67698-ac36-4c1c-8188-e8336e25f023/resourceGroups/rg-Ignite-PREL13/providers/Microsoft.CognitiveServices/accounts/aoai-ob7apaqovowv2/projects/proj-ob7apaqovowv2&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "   üíæ Local results: ./22-evaluate-models-results/22-evaluate-models_gpt-4.1_20251220_054310\n",
      "\n",
      "‚úÖ All evaluations complete!\n",
      "\n",
      "üìÅ All results saved to: ./22-evaluate-models-results/\n",
      "üåê View results in Azure AI Foundry portal using the URLs above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Evaluation Comparison\n",
      "\n",
      "üìä Model Evaluation Comparison\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create output directory for evaluation results\n",
    "output_dir = \"./22-evaluate-models-results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Store results for each model\n",
    "evaluation_results = {}\n",
    "\n",
    "print(f\"üöÄ Starting evaluation of {len(models_to_evaluate)} models...\")\n",
    "print(f\"   Test dataset size: {len(test_data)} examples\")\n",
    "print(f\"   Output directory: {output_dir}\\n\")\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    print(f\"üìä Evaluating model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Create target function for this model\n",
    "        target_fn = create_target_function(model_name)\n",
    "\n",
    "        # Create output path for this evaluation\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_path = os.path.join(\n",
    "            output_dir, f\"22-evaluate-models_{model_name}_{timestamp}\")\n",
    "\n",
    "        # Run evaluation with both portal publishing and local output\n",
    "        result = evaluate(\n",
    "            data=dataset_path,\n",
    "            target=target_fn,\n",
    "            evaluators={\n",
    "                \"relevance\": relevance_eval,\n",
    "                \"coherence\": coherence_eval,\n",
    "                \"fluency\": fluency_eval,\n",
    "                \"violence\": violence_eval,\n",
    "                \"hate_unfairness\": hate_unfairness_eval,\n",
    "                \"self_harm\": self_harm_eval,\n",
    "                \"sexual\": sexual_eval,\n",
    "            },\n",
    "            evaluator_config={\n",
    "                \"default\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"response\": \"${target.response}\",\n",
    "                }\n",
    "            },\n",
    "            # Publish to Azure AI Foundry portal for visualization (using URL format)\n",
    "            azure_ai_project=azure_ai_project_url,\n",
    "            # Save detailed results locally\n",
    "            output_path=output_path,\n",
    "            # Optional: provide evaluation name for easier tracking in portal\n",
    "            evaluation_name=f\"22-evaluate-models_{model_name}_{timestamp}\"\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Store results with both portal and local file information\n",
    "        evaluation_results[model_name] = {\n",
    "            \"metrics\": result[\"metrics\"],\n",
    "            \"evaluation_time\": elapsed_time,\n",
    "            \"studio_url\": result.get(\"studio_url\"),\n",
    "            \"output_path\": output_path\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ Completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"   üìä Portal URL: {result.get('studio_url', 'N/A')}\")\n",
    "        print(f\"   üíæ Local results: {output_path}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error evaluating {model_name}: {str(e)}\\n\")\n",
    "        evaluation_results[model_name] = {\n",
    "            \"error\": str(e),\n",
    "            \"evaluation_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ All evaluations complete!\")\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}/\")\n",
    "print(f\"üåê View results in Azure AI Foundry portal using the URLs above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc2431",
   "metadata": {},
   "source": [
    "## Step 9: Analyze Results\n",
    "\n",
    "Let's create a summary comparison of all models across key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "375046ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Eval Time (s)</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Fluency</th>\n",
       "      <th>Violence</th>\n",
       "      <th>Hate/Unfairness</th>\n",
       "      <th>Self-Harm</th>\n",
       "      <th>Sexual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>58.093849</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>67.854621</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>56.816964</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Eval Time (s)  Relevance  Coherence  Fluency  Violence  \\\n",
       "0  gpt-4o-mini      58.093849        4.8        4.0      3.6       0.0   \n",
       "1       gpt-4o      67.854621        4.6        4.2      3.8       0.0   \n",
       "2      gpt-4.1      56.816964        4.4        4.4      3.6       0.0   \n",
       "\n",
       "   Hate/Unfairness  Self-Harm  Sexual  \n",
       "0              0.0        0.0     0.0  \n",
       "1              0.0        0.0     0.0  \n",
       "2              0.0        0.0     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare data for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    if \"error\" in results:\n",
    "        print(f\"‚ö†Ô∏è  {model_name}: Evaluation failed - {results['error']}\\n\")\n",
    "        continue\n",
    "\n",
    "    metrics = results[\"metrics\"]\n",
    "\n",
    "    # The metrics are stored with keys like \"relevance.relevance\", \"coherence.coherence\", etc.\n",
    "    row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Eval Time (s)\": results['evaluation_time'],\n",
    "        \"Relevance\": metrics.get('relevance.relevance', metrics.get('relevance', 0)),\n",
    "        \"Coherence\": metrics.get('coherence.coherence', metrics.get('coherence', 0)),\n",
    "        \"Fluency\": metrics.get('fluency.fluency', metrics.get('fluency', 0)),\n",
    "        \"Violence\": metrics.get('violence.violence_defect_rate', metrics.get('violence', 0)),\n",
    "        \"Hate/Unfairness\": metrics.get('hate_unfairness.hate_unfairness_defect_rate', metrics.get('hate_unfairness', 0)),\n",
    "        \"Self-Harm\": metrics.get('self_harm.self_harm_defect_rate', metrics.get('self_harm', 0)),\n",
    "        \"Sexual\": metrics.get('sexual.sexual_defect_rate', metrics.get('sexual', 0)),\n",
    "    }\n",
    "\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"üìä Model Evaluation Comparison\\n\")\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088ee16",
   "metadata": {},
   "source": [
    "## Step 10: Performance Summary\n",
    "\n",
    "Analyze the evaluation results to identify the best performing models across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86d089ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üèÜ Best Performing Models by Metric"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Best Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relevance</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>4.800</td>\n",
       "      <td>‚Üë Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coherence</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4.400</td>\n",
       "      <td>‚Üë Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fluency</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>3.800</td>\n",
       "      <td>‚Üë Higher is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Violence Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>‚Üì Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hate/Unfairness Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>‚Üì Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Self-Harm Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>‚Üì Lower is Better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sexual Safety</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.000</td>\n",
       "      <td>‚Üì Lower is Better</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Metric   Best Model  Score           Direction\n",
       "0               Relevance  gpt-4o-mini  4.800  ‚Üë Higher is Better\n",
       "1               Coherence      gpt-4.1  4.400  ‚Üë Higher is Better\n",
       "2                 Fluency       gpt-4o  3.800  ‚Üë Higher is Better\n",
       "3         Violence Safety  gpt-4o-mini  0.000   ‚Üì Lower is Better\n",
       "4  Hate/Unfairness Safety  gpt-4o-mini  0.000   ‚Üì Lower is Better\n",
       "5        Self-Harm Safety  gpt-4o-mini  0.000   ‚Üì Lower is Better\n",
       "6           Sexual Safety  gpt-4o-mini  0.000   ‚Üì Lower is Better"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## üåü Overall Best Model (Quality Metrics Average)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ü•á **gpt-4o** - Quality: 4.200 | Time: 67.85s"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg Quality Score</th>\n",
       "      <th>Eval Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ü•á 1</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>4.200</td>\n",
       "      <td>67.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ü•à 2</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>4.133</td>\n",
       "      <td>58.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ü•â 3</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>4.133</td>\n",
       "      <td>56.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank        Model Avg Quality Score Eval Time (s)\n",
       "0  ü•á 1       gpt-4o             4.200         67.85\n",
       "1  ü•à 2  gpt-4o-mini             4.133         58.09\n",
       "2  ü•â 3      gpt-4.1             4.133         56.82"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Check for successful evaluations\n",
    "successful_models = {name: results for name,\n",
    "                     results in evaluation_results.items() if 'error' not in results}\n",
    "failed_models = {name: results for name,\n",
    "                 results in evaluation_results.items() if 'error' in results}\n",
    "\n",
    "if successful_models:\n",
    "    display(Markdown(\"## üèÜ Best Performing Models by Metric\"))\n",
    "\n",
    "    # Define evaluator metrics and their optimization direction\n",
    "    evaluator_metrics = [\n",
    "        ('relevance.relevance', 'Relevance', True),\n",
    "        ('coherence.coherence', 'Coherence', True),\n",
    "        ('fluency.fluency', 'Fluency', True),\n",
    "        ('violence.violence_defect_rate', 'Violence Safety', False),\n",
    "        ('hate_unfairness.hate_unfairness_defect_rate',\n",
    "         'Hate/Unfairness Safety', False),\n",
    "        ('self_harm.self_harm_defect_rate', 'Self-Harm Safety', False),\n",
    "        ('sexual.sexual_defect_rate', 'Sexual Safety', False),\n",
    "    ]\n",
    "\n",
    "    # Create a dataframe for best models\n",
    "    best_models_data = []\n",
    "\n",
    "    for metric_key, display_name, higher_is_better in evaluator_metrics:\n",
    "        valid_models = {}\n",
    "\n",
    "        # Collect scores for this metric from all successful models\n",
    "        for model_name, results in successful_models.items():\n",
    "            metrics = results['metrics']\n",
    "            score = metrics.get(metric_key)\n",
    "            if score is not None:\n",
    "                valid_models[model_name] = score\n",
    "\n",
    "        if valid_models:\n",
    "            # Find best model based on optimization direction\n",
    "            if higher_is_better:\n",
    "                best_model_name = max(valid_models, key=valid_models.get)\n",
    "                best_score = valid_models[best_model_name]\n",
    "                direction = \"‚Üë Higher is Better\"\n",
    "            else:\n",
    "                best_model_name = min(valid_models, key=valid_models.get)\n",
    "                best_score = valid_models[best_model_name]\n",
    "                direction = \"‚Üì Lower is Better\"\n",
    "\n",
    "            best_models_data.append({\n",
    "                \"Metric\": display_name,\n",
    "                \"Best Model\": best_model_name,\n",
    "                \"Score\": f\"{best_score:.3f}\",\n",
    "                \"Direction\": direction\n",
    "            })\n",
    "\n",
    "    df_best = pd.DataFrame(best_models_data)\n",
    "    display(df_best)\n",
    "\n",
    "    # Calculate overall best model (based on quality metrics average)\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"## üåü Overall Best Model (Quality Metrics Average)\"))\n",
    "\n",
    "    quality_metric_keys = ['relevance.relevance',\n",
    "                           'coherence.coherence', 'fluency.fluency']\n",
    "    model_quality_scores = {}\n",
    "\n",
    "    for model_name, results in successful_models.items():\n",
    "        metrics = results['metrics']\n",
    "        scores = []\n",
    "        for metric_key in quality_metric_keys:\n",
    "            score = metrics.get(metric_key)\n",
    "            if score is not None:\n",
    "                scores.append(score)\n",
    "\n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            model_quality_scores[model_name] = {\n",
    "                'avg_quality': avg_score,\n",
    "                'eval_time': results['evaluation_time']\n",
    "            }\n",
    "\n",
    "    if model_quality_scores:\n",
    "        best_overall = max(model_quality_scores,\n",
    "                           key=lambda x: model_quality_scores[x]['avg_quality'])\n",
    "        best_data = model_quality_scores[best_overall]\n",
    "\n",
    "        display(Markdown(\n",
    "            f\"ü•á **{best_overall}** - Quality: {best_data['avg_quality']:.3f} | Time: {best_data['eval_time']:.2f}s\"))\n",
    "\n",
    "        # Show all model quality scores for comparison\n",
    "        display(Markdown(\"\"))\n",
    "\n",
    "        ranking_data = []\n",
    "        sorted_models = sorted(model_quality_scores.items(\n",
    "        ), key=lambda x: x[1]['avg_quality'], reverse=True)\n",
    "\n",
    "        for rank, (model_name, data) in enumerate(sorted_models, 1):\n",
    "            medal = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"\"\n",
    "            ranking_data.append({\n",
    "                \"Rank\": f\"{medal} {rank}\",\n",
    "                \"Model\": model_name,\n",
    "                \"Avg Quality Score\": f\"{data['avg_quality']:.3f}\",\n",
    "                \"Eval Time (s)\": f\"{data['eval_time']:.2f}\"\n",
    "            })\n",
    "\n",
    "        df_ranking = pd.DataFrame(ranking_data)\n",
    "        display(df_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5dd01",
   "metadata": {},
   "source": [
    "## Step 11: Next Steps\n",
    "\n",
    "You've successfully evaluated multiple models! Here are some next steps to consider:\n",
    "\n",
    "### üìä View Results in Two Places\n",
    "\n",
    "- **Azure AI Foundry Portal**: Interactive visualizations with [detailed charts and comparisons](https://learn.microsoft.com/azure/ai-foundry/how-to/evaluate-results)\n",
    "- **Portal URLs**: Each evaluation includes a studio URL for easy access and team sharing\n",
    "- **Local Files**: All results saved in `./22-evaluate-models-results/` for offline analysis\n",
    "- **Version Control**: Commit JSON files for reproducibility and tracking over time\n",
    "\n",
    "### üèÜ Use Model Leaderboards for Selection\n",
    "\n",
    "- **Browse Leaderboards**: Compare models by [Quality, Safety, Cost, and Performance](https://learn.microsoft.com/azure/ai-foundry/how-to/benchmark-model-in-catalog)\n",
    "- **Trade-off Analysis**: View quality vs. cost, quality vs. safety charts\n",
    "- **Scenario Filtering**: Find models best suited for your use case (Q&A, coding, reasoning)\n",
    "- **Access Portal**: [Azure AI Foundry Model Catalog ‚Üí Browse Leaderboards](https://aka.ms/model-leaderboards)\n",
    "\n",
    "---\n",
    "\n",
    "**Great work! You now have comprehensive evaluation metrics for multiple models.** üéâ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
